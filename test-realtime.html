<!DOCTYPE html>
<html lang="fa" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ØªØ³Øª Real-time Speech Recognition</title>
    <style>
        body {
            font-family: 'Tahoma', sans-serif;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .button {
            padding: 15px 30px;
            margin: 10px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
            font-family: inherit;
        }
        .start-btn {
            background: #4CAF50;
            color: white;
        }
        .stop-btn {
            background: #f44336;
            color: white;
        }
        .status {
            padding: 10px;
            margin: 10px 0;
            border-radius: 5px;
            font-weight: bold;
        }
        .listening {
            background: #e8f5e8;
            color: #2e7d32;
        }
        .stopped {
            background: #ffebee;
            color: #c62828;
        }
        #transcript {
            width: 100%;
            height: 200px;
            padding: 15px;
            border: 2px solid #ddd;
            border-radius: 5px;
            font-size: 16px;
            line-height: 1.6;
            resize: vertical;
        }
        .audio-level {
            width: 100%;
            height: 20px;
            background: #ddd;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }
        .audio-bar {
            height: 100%;
            background: linear-gradient(90deg, #4CAF50, #FFC107, #FF5722);
            width: 0%;
            transition: width 0.1s;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¤ ØªØ³Øª Real-time Speech Recognition</h1>
        
        <div id="status" class="status stopped">
            Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹
        </div>
        
        <div>
            <button id="startBtn" class="button start-btn">ğŸ™ï¸ Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ ØµÙˆØª</button>
            <button id="stopBtn" class="button stop-btn" disabled>â¹ï¸ ØªÙˆÙ‚Ù</button>
            <button id="clearBtn" class="button">ğŸ—‘ï¸ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù†</button>
        </div>
        
        <div class="audio-level">
            <div id="audioBar" class="audio-bar"></div>
        </div>
        
        <h3>Ù…ØªÙ† ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡:</h3>
        <textarea id="transcript" placeholder="Ù…ØªÙ† ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯..."></textarea>
        
        <div id="info" style="margin-top: 20px; padding: 15px; background: #e3f2fd; border-radius: 5px;">
            <h4>ğŸ“‹ Ø±Ø§Ù‡Ù†Ù…Ø§:</h4>
            <ul>
                <li>Ø±ÙˆÛŒ "Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ ØµÙˆØª" Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯</li>
                <li>Ù…Ø¬ÙˆØ² Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø±Ø§ Ø¨Ø¯Ù‡ÛŒØ¯</li>
                <li>Ø´Ø±ÙˆØ¹ Ø¨Ù‡ ØµØ­Ø¨Øª Ú©Ù†ÛŒØ¯</li>
                <li>Ù…ØªÙ† Ø¨Ù‡ ØµÙˆØ±Øª Real-time Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯</li>
                <li>Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ØŒ ÙˆØ§Ø¶Ø­ Ùˆ Ø¢Ù‡Ø³ØªÙ‡ ØµØ­Ø¨Øª Ú©Ù†ÛŒØ¯</li>
            </ul>
        </div>
    </div>

    <script>
        let recognition = null;
        let isListening = false;
        let audioContext = null;
        let microphone = null;
        let analyser = null;
        let animationFrame = null;

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const clearBtn = document.getElementById('clearBtn');
        const status = document.getElementById('status');
        const transcript = document.getElementById('transcript');
        const audioBar = document.getElementById('audioBar');

        // Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Web Speech API
        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
            status.textContent = 'âŒ Ù…Ø±ÙˆØ±Ú¯Ø± Ø´Ù…Ø§ Ø§Ø² ØªØ´Ø®ÛŒØµ ØµÙˆØª Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯';
            status.className = 'status stopped';
            startBtn.disabled = true;
        } else {
            console.log('âœ… Web Speech API Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯');
        }

        // Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ ØµÙˆØª
        startBtn.addEventListener('click', async () => {
            try {
                // Ø§ÛŒØ¬Ø§Ø¯ ØªØ´Ø®ÛŒØµ ØµÙˆØª
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();

                recognition.continuous = true;
                recognition.interimResults = true;
                recognition.lang = 'fa-IR';
                recognition.maxAlternatives = 1;

                // Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§
                recognition.onstart = () => {
                    isListening = true;
                    status.textContent = 'ğŸ¤ Ø¯Ø± Ø­Ø§Ù„ Ú¯ÙˆØ´ Ø¯Ø§Ø¯Ù†...';
                    status.className = 'status listening';
                    startBtn.disabled = true;
                    stopBtn.disabled = false;
                    console.log('Speech recognition started');
                };

                recognition.onresult = (event) => {
                    let finalTranscript = '';
                    let interimTranscript = '';

                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const result = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += result + ' ';
                        } else {
                            interimTranscript += result;
                        }
                    }

                    if (finalTranscript) {
                        transcript.value += finalTranscript;
                        console.log('Final:', finalTranscript);
                    }

                    if (interimTranscript) {
                        console.log('Interim:', interimTranscript);
                    }
                };

                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                    status.textContent = `âŒ Ø®Ø·Ø§: ${event.error}`;
                    status.className = 'status stopped';
                    
                    if (event.error === 'not-allowed') {
                        alert('Ù„Ø·ÙØ§Ù‹ Ù…Ø¬ÙˆØ² Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø±Ø§ Ø¨Ø¯Ù‡ÛŒØ¯');
                    }
                };

                recognition.onend = () => {
                    if (isListening) {
                        // Ø´Ø±ÙˆØ¹ Ù…Ø¬Ø¯Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ù…Ù‡ ØªØ´Ø®ÛŒØµ
                        try {
                            recognition.start();
                        } catch (error) {
                            console.log('Restart error:', error);
                        }
                    }
                };

                // Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ
                recognition.start();

                // Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³Ø·Ø­ ØµØ¯Ø§
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                
                analyser.fftSize = 256;
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                
                microphone.connect(analyser);

                // Ù†Ù…Ø§ÛŒØ´ Ø³Ø·Ø­ ØµØ¯Ø§
                const updateAudioLevel = () => {
                    if (isListening) {
                        analyser.getByteFrequencyData(dataArray);
                        const average = dataArray.reduce((a, b) => a + b) / bufferLength;
                        const level = Math.min(100, (average / 255) * 100);
                        audioBar.style.width = level + '%';
                        animationFrame = requestAnimationFrame(updateAudioLevel);
                    }
                };
                updateAudioLevel();

            } catch (error) {
                console.error('Error:', error);
                alert('Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ ØµÙˆØª: ' + error.message);
            }
        });

        // ØªÙˆÙ‚Ù ØªØ´Ø®ÛŒØµ ØµÙˆØª
        stopBtn.addEventListener('click', () => {
            isListening = false;
            
            if (recognition) {
                recognition.stop();
            }
            
            if (audioContext) {
                audioContext.close();
            }
            
            if (animationFrame) {
                cancelAnimationFrame(animationFrame);
            }

            status.textContent = 'â¹ï¸ ØªØ´Ø®ÛŒØµ ØµÙˆØª Ù…ØªÙˆÙ‚Ù Ø´Ø¯';
            status.className = 'status stopped';
            startBtn.disabled = false;
            stopBtn.disabled = true;
            audioBar.style.width = '0%';
        });

        // Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù…ØªÙ†
        clearBtn.addEventListener('click', () => {
            transcript.value = '';
        });
    </script>
</body>
</html>
